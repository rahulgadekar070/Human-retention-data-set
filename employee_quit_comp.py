# -*- coding: utf-8 -*-
"""employee_Quit_comp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Td7LrEei_lAbQ11ZtMqxhYloq2yP9Plz
"""

# Objective: To build a predictive model which determines whether your Employee will Quit your Company or not.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""There are 2 data files one is 'hr_data' & other is 'employee_satisfaction_data'

*Load HR data...*
"""

hr_data = pd.read_csv('/content/drive/My Drive/#Data Science/Advanced projects/Employee_Quit_Compny/hr_data.csv')

"""# **Numerical Analysis / EDA**"""

hr_data
#In this, 'left' column is our Target/Output variable...which shows whether employee left or not
# 0=Not left, 1=left company

hr_data.describe

hr_data.columns

hr_data.shape

hr_data.size

hr_data.info()   # OR
# hr_data.isnull().sum()    #No any 'Null' values

"""Unique values..."""

hr_data['department'].unique()

hr_data['salary'].unique()

"""*Load Employee Satisfaction Data...*"""

sat_data = pd.read_excel('/content/drive/My Drive/#Data Science/Advanced projects/Employee_Quit_Compny/employee_satisfaction_evaluation.xlsx')

sat_data

sat_data.columns

sat_data.info()

sat_data.isnull().sum()

"""**Merging & Joining Both Files...**"""

main_df = hr_data.set_index('employee_id').join(sat_data.set_index('EMPLOYEE #'))

main_df

main_df = main_df.reset_index()   #Bcoz index not looking good so reset it.

main_df

"""Check Null Values..."""

main_df.isnull().sum()

main_df[main_df.isnull().any(axis=1)]   #Null value w.r.t. that specific row(axis=1)

"""Fill **'Null'** values by their **mean**..."""

main_df.describe()

main_df.fillna(main_df.mean(),inplace=True)  #fill the mean values w.r.t. their columns.

main_df[main_df.isnull().any(axis=1)]   #'0' Null value here...

main_df.loc[main_df['employee_id']==3794]   # for cross check the 'NA' value.

main_df.drop(columns='employee_id',inplace=True)   #remove unwanted column...

main_df

main_df.groupby('department').mean()

main_df.groupby('department').sum()

main_df['department'].value_counts()

main_df['left'].value_counts()

"""# **Data Visualization...**"""

import matplotlib.pyplot as plt
import seaborn as sns

"""Correlation Matrix..."""

def plot_corr(df,size=10):
    
    corr=df.corr()
    fig,ax=plt.subplots(figsize=(size,size))
    ax.legend()
    cax=ax.matshow(corr)
    fig.colorbar(cax)
    plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical')
    plt.yticks(range(len(corr.columns)), corr.columns)
    

plot_corr(main_df)

# from plot we saw that 'satisfaction_level' is most affected factor...bcoz less satisfacyed &
# 'avg_month_hrs' not effected mostly.

"""Barplot..."""

sns.barplot(x='left',y='satisfaction_level',data=main_df)

# from plot it shows that, when 'satis_level' is above 60% employee not leave the job &
# when 'satis_level' is below the 50% employee leave the job.

sns.barplot(x='promotion_last_5years',y='satisfaction_level',data=main_df)

# here '0'=not promoted, '1'=promoted 
# we show that, when 'sats_level' is greater employees promoted

"""# **Data Preprocessing...**

Label Encoder...(To convert string into Numerical)
"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

# 'salary' & 'department' both are string columns...

s = le.fit_transform(main_df['salary'])

s

main_df['salary_n'] = s  #make 'salary_n' as new column which is array(numerical) in 'main_df' data.

main_df

main_df.loc[main_df['salary']=='low']  # for cross check purpose '0'=high, '1'=low.

# same for 'department' column...

d = le.fit_transform(main_df['department'])

d    # there are 8 depts so array upto '8'.

main_df['dept'] = d

main_df

main_df.loc[main_df['department']=='hr']     # cross-check 'sales'=7, 'IT'=0, 'hr'=3.

"""Then remove unwanted original columns..."""

main_df.drop(['department'],axis=1, inplace=True)

main_df.drop(['salary'],axis=1,inplace=True)

main_df

x = main_df.drop(['left'],axis=1)   #'x'=i/p variable & 'left' is our o/p variable so seperate it.

x

y = main_df['left']   # 'y'=o/p variable

y

"""# **Train & Test Split...**"""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)

x_test

y_test

"""# **Model Building...**

# ***Decision Tree Classification Model***
"""

from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(x_train,y_train)

predict_dt = dt.predict(x_test)

predict_dt

y_test

"""**Results...**"""

from sklearn.metrics import classification_report,f1_score
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,predict_dt))
print('\n')
print(classification_report(y_test,predict_dt))

"""**Accuracy**"""

acc_dt = accuracy_score(y_test,predict_dt)*100    #'y_test' is actual data
acc_dt
 # 97.73%

"""Find out whether employee will stay or not by putting actual values..."""

x_test

category = ['Employee will stay', 'Employee will leave']   #'0'=stay & '1'=leave

# '9' columns so put 9 respective values
custom_dt = [[1,500,3,1,0,0.89,0.78,2,7]]   #double bracket for 2D array

print(int(dt.predict(custom_dt)))     #convert array into 'int'

category[int(dt.predict(custom_dt))]

"""Find which is the most important feature..."""

dt.feature_importances_    #it results in array...

feature_impo = pd.DataFrame(dt.feature_importances_,index=x_train.columns,columns=['Importance']).sort_values('Importance',ascending=False)   
# converted array into dataframe, 'Importance' is just a name of new column to understand.

feature_impo
 # 'satisfaction_level' is most effected feature (0.5083)
 # then 'last_evaluation' (0.1473)

"""# ***KNN Model***

***Pre-processing...Use Standard Scaler***(Bcoz we want to find out distance, not an only yes/no type o/p in KNN)
"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler().fit(x_train)
x_train_std = sc.transform(x_train)
x_test_std  = sc.transform(x_test)

x_train_std      # all values in between 0 & 1 by using StdScaler.

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train_std,y_train)

predict_knn = knn.predict(x_test_std)
predict_knn    # predicted results

y_test     # Actual results/values...compared it with predicted results.

"""**Results**"""

print(confusion_matrix(y_test,predict_knn))
print('\n')
print(classification_report(y_test,predict_knn))

"""**Accuracy**"""

acc_knn = accuracy_score(y_test,predict_knn)*100
acc_knn
 # 95.86%

"""Here Find Best 'k' value..."""

k_range = range(1,26)
scores = {}
scores_list = []

for k in k_range:
  knn = KNeighborsClassifier(n_neighbors=k)
  knn.fit(x_train_std,y_train)
  predict_knn = knn.predict(x_test_std)
  scores[k] = accuracy_score(y_test,predict_knn)*100
  scores_list.append(accuracy_score(y_test,predict_knn))

scores

scores_list  #we want to plot the data so we use list to avail all data.

plt.plot(k_range,scores_list)
 # from plot shows that, k=1 gives best acuuracy but we goes with k=3 atleast.

"""Find out whether employee will stay or not by putting actual values on custom_data..."""

category = ['Employee will stay', 'Employee will leave']

knn_custom = [[20,500,10,6,0,0.10,0.30,1,8]]
knn_custom_StdScal = sc.transform(knn_custom)  #convert this custom data also in StdScaler.

knn_custom_StdScal

print(int(knn.predict(knn_custom_StdScal)))

category[int(knn.predict(knn_custom_StdScal))]

"""same for most effected factor as did in 'Decision Tree'

**Accuracy Plots of 'Decision Tree' & 'KNN' Algorithms ****
"""

algorithms = ['Decision Tree', 'KNN']
scores = [acc_dt, acc_knn]
plt.xlabel('Algorithms')
plt.ylabel('Accuracy Score')
sns.barplot(algorithms, scores)
plt.show()

##### Comment :-
# By using 'Decision Tree' & 'KNN' classifier we got 97.73% & 95.86% Accuracy respectively.
